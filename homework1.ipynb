{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFORMATION RETRIEVAL HOMEWORK 1\n",
    "\n",
    "##### Link to google doc: https://docs.google.com/document/d/1JeqVB5HVqv3lq5GrltYqbq9dmbQpUf_NCNpow78WEBI/edit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Part (15 pts)\n",
    "\n",
    "----\n",
    "\n",
    "#### Hypothesis Testing – The problem of multiple comparisons [5 points]\n",
    "\n",
    "Experimentation in AI often happens like this:\n",
    "1. Modify/Build an algorithm\n",
    "2. Compare the algorithm to a baseline by running a hypothesis test.\n",
    "3. If not significant, go back to step 1.\n",
    "4. If significant, start writing a paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many hypothesis tests, m, does it take to get to (with Type I error for each test = α):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P($m^{th}$ experiment gives significant result | m experiments lacking power to reject $H_0$)?\n",
    "\n",
    "\n",
    "\n",
    "#### TODO: YOUR ANSWER HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(at least one significant result | m experiments lacking power to reject $H_0$)?\n",
    "\n",
    "\n",
    "#### TODO: YOUR ANSWER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias and unfairness in Interleaving experiments (10 points)\n",
    "\n",
    "\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning $\\frac{2}{3}$ of the time, even though in theory the percentage of wins should be 50% for both algorithms. \n",
    "\n",
    "__Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental Part [85 pts]\n",
    "\n",
    "---\n",
    "\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (*P*) with the new experimental algorithm (*E*); if *E* statistically significantly outperforms *P* with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.\n",
    "\n",
    "\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:\n",
    "1. Binary evaluation measures\n",
    "    * Precision at rank k,\n",
    "    * Recall at rank k,\n",
    "    * Average Precision,\n",
    "2. Multi-graded evaluation measures\n",
    "    * Normalized Discounted Cumulative Gain at rank k (nDCG@k),\n",
    "    * Expected Reciprocal Rank (ERR).\n",
    "\n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "1. Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.),\n",
    "2. Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).\n",
    " \n",
    "In an interleaving experiment the ranked results of *P* and *E* (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for *P* and *E* are computed. \n",
    "\n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the *E* wins and test whether this proportion, *p*, is greater than $p_0=0.5$. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "\n",
    "One of the key questions however is **whether offline evaluation and online evaluation outcomes agree with each other**. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. “Large-Scale Validation and Analysis of Interleaved Search Evaluation”.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [Based on Lecture 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Step 1: Simulate Rankings of Relevance for E and P [5 points]\n",
    "\n",
    "In the first step you will generate pairs of rankings of relevance, for the production *P* and experimental *E*, respectively, for a hypothetical query **q**. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible *P* and *E* ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "\n",
    "Example:\n",
    "* P: {N N N N N}; E: {N N N N R}\n",
    "* …\n",
    "* P: {HR HR HR HR R}; E: {HR HR HR HR HR}\n",
    "\n",
    "\n",
    "(Note 1: If you do not have enough computational power, sample 5000 pair uniformly at random to show your work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "relevance = ['N', 'R', 'HR']\n",
    "single_ranking = list(product(relevance, repeat = 5))\n",
    "rankings = list(product(single_ranking, repeat = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
